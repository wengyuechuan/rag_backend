"""
æ–‡æ¡£å¤„ç†æœåŠ¡ - æ”¯æŒå¼‚æ­¥å¤„ç†
åŒ…æ‹¬æ–‡æ¡£åˆ†å—ã€å‘é‡åŒ–ã€å®ä½“å…³ç³»æŠ½å–ç­‰åŠŸèƒ½
"""

import time
import threading
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from sqlalchemy.orm import Session
from typing import List, Optional, Tuple, Dict, Any
import numpy as np
import os
import json

from database.models import Document, DocumentChunk, KnowledgeBase, DocumentStatusEnum, ChunkStrategyEnum
from database.database import SessionLocal
from utils.chunk import TextChunker
from utils.faiss import FaissVectorStore
from api.schemas import DocumentCreate

# å¯¼å…¥ NER å’Œ Neo4j
try:
    from utils.chunk_to_ner import EntityRelationExtractor
    NER_AVAILABLE = True
except Exception as e:
    NER_AVAILABLE = False
    print(f"âš ï¸  NER åŠŸèƒ½ä¸å¯ç”¨: {e}")

try:
    from utils.neo4j import Neo4jKnowledgeGraph
    NEO4J_AVAILABLE = True
except Exception as e:
    NEO4J_AVAILABLE = False
    print(f"âš ï¸  Neo4j åŠŸèƒ½ä¸å¯ç”¨: {e}")


class DocumentProcessingService:
    """æ–‡æ¡£å¤„ç†æœåŠ¡ - æ”¯æŒå¼‚æ­¥åå°å¤„ç†"""
    
    def __init__(self, max_workers: int = 4):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†æœåŠ¡
        
        Args:
            max_workers: çº¿ç¨‹æ± æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        self.vector_stores = {}  # ç¼“å­˜å‘é‡å­˜å‚¨å®ä¾‹ {kb_id: FaissVectorStore}
        self.ner_extractors = {}  # ç¼“å­˜ NER æå–å™¨ {kb_id: EntityRelationExtractor}
        self.neo4j_clients = {}   # ç¼“å­˜ Neo4j å®¢æˆ·ç«¯ {kb_id: Neo4jKnowledgeGraph}
        
        # çº¿ç¨‹æ± ç”¨äºå¼‚æ­¥å¤„ç†
        self.executor = ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="doc_processor")
        self.processing_lock = threading.Lock()
        self.processing_tasks = {}  # {document_id: Future}
        
        print(f"âœ… æ–‡æ¡£å¤„ç†æœåŠ¡å·²åˆå§‹åŒ– (æœ€å¤§å·¥ä½œçº¿ç¨‹: {max_workers})")
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        try:
            self.executor.shutdown(wait=False)
        except:
            pass
    
    # ==================== èµ„æºè·å– ====================
    
    def get_vector_store(self, kb: KnowledgeBase) -> Optional[FaissVectorStore]:
        """è·å–æˆ–åˆ›å»ºå‘é‡å­˜å‚¨å®ä¾‹"""
        if not kb.enable_vector_store:
            return None
        
        if kb.id not in self.vector_stores:
            try:
                self.vector_stores[kb.id] = FaissVectorStore(
                    embedding_model=kb.embedding_model,
                    index_type="Flat",
                    metric="Cosine"
                )
                print(f"âœ… ä¸ºçŸ¥è¯†åº“ {kb.id} åˆ›å»ºå‘é‡å­˜å‚¨")
            except Exception as e:
                print(f"âŒ åˆ›å»ºå‘é‡å­˜å‚¨å¤±è´¥: {e}")
                return None
        
        return self.vector_stores[kb.id]
    
    def get_ner_extractor(self, kb: KnowledgeBase) -> Optional[EntityRelationExtractor]:
        """è·å–æˆ–åˆ›å»º NER æå–å™¨"""
        if not kb.enable_ner or not NER_AVAILABLE:
            return None
        
        if kb.id not in self.ner_extractors:
            try:
                api_key = os.getenv("OPENAI_API_KEY")
                base_url = os.getenv("OPENAI_BASE_URL")
                
                if not api_key:
                    print("âš ï¸  æœªè®¾ç½® OPENAI_API_KEYï¼ŒNER åŠŸèƒ½ä¸å¯ç”¨")
                    return None
                
                self.ner_extractors[kb.id] = EntityRelationExtractor(
                    api_key=api_key,
                    base_url=base_url,
                    model=os.getenv("OPENAI_MODEL", "gpt-4"),
                    temperature=0.3
                )
                print(f"âœ… ä¸ºçŸ¥è¯†åº“ {kb.id} åˆ›å»º NER æå–å™¨")
            except Exception as e:
                print(f"âŒ åˆ›å»º NER æå–å™¨å¤±è´¥: {e}")
                return None
        
        return self.ner_extractors[kb.id]
    
    def get_neo4j_client(self, kb: KnowledgeBase) -> Optional[Neo4jKnowledgeGraph]:
        """è·å–æˆ–åˆ›å»º Neo4j å®¢æˆ·ç«¯"""
        if not kb.enable_knowledge_graph or not NEO4J_AVAILABLE:
            return None
        
        if kb.id not in self.neo4j_clients:
            try:
                uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
                username = os.getenv("NEO4J_USERNAME", "neo4j")
                password = os.getenv("NEO4J_PASSWORD", "password")
                
                self.neo4j_clients[kb.id] = Neo4jKnowledgeGraph(
                    uri=uri,
                    username=username,
                    password=password
                )
                print(f"âœ… ä¸ºçŸ¥è¯†åº“ {kb.id} åˆ›å»º Neo4j å®¢æˆ·ç«¯")
            except Exception as e:
                print(f"âŒ åˆ›å»º Neo4j å®¢æˆ·ç«¯å¤±è´¥: {e}")
                return None
        
        return self.neo4j_clients[kb.id]
    
    # ==================== å¼‚æ­¥å¤„ç†æ¥å£ ====================
    
    def process_document_async(self, document_id: int, kb_id: int):
        """
        å¼‚æ­¥å¤„ç†æ–‡æ¡£ï¼ˆæäº¤åˆ°çº¿ç¨‹æ± ï¼‰
        
        Args:
            document_id: æ–‡æ¡£ID
            kb_id: çŸ¥è¯†åº“ID
        """
        with self.processing_lock:
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨å¤„ç†ä¸­
            if document_id in self.processing_tasks:
                existing_task = self.processing_tasks[document_id]
                if not existing_task.done():
                    print(f"âš ï¸  æ–‡æ¡£ {document_id} å·²åœ¨å¤„ç†é˜Ÿåˆ—ä¸­")
                    return
            
            # æäº¤æ–°ä»»åŠ¡
            future = self.executor.submit(
                self._process_document_worker,
                document_id,
                kb_id
            )
            self.processing_tasks[document_id] = future
            print(f"ğŸ“‹ æ–‡æ¡£ {document_id} å·²æäº¤åˆ°å¤„ç†é˜Ÿåˆ—")
    
    def _process_document_worker(self, document_id: int, kb_id: int):
        """
        åå°å·¥ä½œçº¿ç¨‹ - å¤„ç†æ–‡æ¡£
        
        è¿™ä¸ªæ–¹æ³•åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œï¼Œä¸ä¼šé˜»å¡ä¸»çº¿ç¨‹
        """
        # åˆ›å»ºæ–°çš„æ•°æ®åº“ session
        db = SessionLocal()
        
        try:
            print(f"\n{'='*60}")
            print(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£ {document_id}")
            print(f"{'='*60}")
            
            start_time = time.time()
            
            # è·å–æ–‡æ¡£å’ŒçŸ¥è¯†åº“
            document = db.query(Document).filter(Document.id == document_id).first()
            kb = db.query(KnowledgeBase).filter(KnowledgeBase.id == kb_id).first()
            
            if not document or not kb:
                print(f"âŒ æ–‡æ¡£ {document_id} æˆ–çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
                return
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            document.status = DocumentStatusEnum.PROCESSING
            db.commit()
            
            # 1. æ–‡æ¡£åˆ†å—
            print(f"\n[æ­¥éª¤ 1/4] æ–‡æ¡£åˆ†å—...")
            chunks = self._chunk_document_step(db, document, kb)
            
            if not chunks:
                raise ValueError("æ–‡æ¡£åˆ†å—å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆä»»ä½•åˆ†å—")
            
            print(f"âœ… ç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")
            
            # 2. å‘é‡åŒ–
            if kb.enable_vector_store:
                print(f"\n[æ­¥éª¤ 2/4] å‘é‡åŒ–...")
                self._vectorize_chunks_step(db, chunks, kb)
                print(f"âœ… å‘é‡åŒ–å®Œæˆ")
                document.vector_stored = True
            else:
                print(f"\n[æ­¥éª¤ 2/4] è·³è¿‡å‘é‡åŒ–ï¼ˆæœªå¯ç”¨ï¼‰")
            
            # 3. å®ä½“å…³ç³»æå–
            if kb.enable_ner:
                print(f"\n[æ­¥éª¤ 3/4] å®ä½“å…³ç³»æå–...")
                entity_count, relation_count = self._extract_entities_relations_step(
                    db, document, chunks, kb
                )
                print(f"âœ… æå–äº† {entity_count} ä¸ªå®ä½“ï¼Œ{relation_count} ä¸ªå…³ç³»")
                document.entity_count = entity_count
                document.relation_count = relation_count
            else:
                print(f"\n[æ­¥éª¤ 3/4] è·³è¿‡å®ä½“å…³ç³»æå–ï¼ˆæœªå¯ç”¨ï¼‰")
            
            # 4. çŸ¥è¯†å›¾è°±å­˜å‚¨
            if kb.enable_knowledge_graph and kb.enable_ner:
                print(f"\n[æ­¥éª¤ 4/4] çŸ¥è¯†å›¾è°±å­˜å‚¨...")
                self._store_to_knowledge_graph_step(db, chunks, kb)
                print(f"âœ… çŸ¥è¯†å›¾è°±å­˜å‚¨å®Œæˆ")
                document.graph_stored = True
            else:
                print(f"\n[æ­¥éª¤ 4/4] è·³è¿‡çŸ¥è¯†å›¾è°±å­˜å‚¨ï¼ˆæœªå¯ç”¨ï¼‰")
            
            # æ›´æ–°æ–‡æ¡£ç»Ÿè®¡
            document.chunk_count = len(chunks)
            document.char_count = len(document.content)
            document.word_count = len(document.content.split())
            document.status = DocumentStatusEnum.COMPLETED
            document.processing_time = time.time() - start_time
            document.processed_at = datetime.utcnow()
            
            # æ›´æ–°çŸ¥è¯†åº“ç»Ÿè®¡
            kb.document_count += 1
            kb.total_chunks += len(chunks)
            
            db.commit()
            
            print(f"\n{'='*60}")
            print(f"âœ… æ–‡æ¡£ {document_id} å¤„ç†å®Œæˆ")
            print(f"   è€—æ—¶: {document.processing_time:.2f}ç§’")
            print(f"   åˆ†å—æ•°: {len(chunks)}")
            print(f"   å‘é‡åŒ–: {'æ˜¯' if document.vector_stored else 'å¦'}")
            print(f"   çŸ¥è¯†å›¾è°±: {'æ˜¯' if document.graph_stored else 'å¦'}")
            print(f"{'='*60}\n")
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†æ–‡æ¡£ {document_id} å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            
            # æ›´æ–°é”™è¯¯çŠ¶æ€
            try:
                document = db.query(Document).filter(Document.id == document_id).first()
                if document:
                    document.status = DocumentStatusEnum.FAILED
                    document.error_message = str(e)
                    db.commit()
            except:
                pass
        
        finally:
            db.close()
            
            # æ¸…ç†ä»»åŠ¡è®°å½•
            with self.processing_lock:
                if document_id in self.processing_tasks:
                    del self.processing_tasks[document_id]
    
    # ==================== å¤„ç†æ­¥éª¤ ====================
    
    def _chunk_document_step(
        self,
        db: Session,
        document: Document,
        kb: KnowledgeBase
    ) -> List[DocumentChunk]:
        """æ–‡æ¡£åˆ†å—æ­¥éª¤"""
        # ç¡®å®šåˆ†å—é…ç½®
        chunk_strategy = document.chunk_strategy or kb.default_chunk_strategy
        chunk_size = document.chunk_size or kb.default_chunk_size
        chunk_overlap = document.chunk_overlap or kb.default_chunk_overlap
        
        # æ‰§è¡Œåˆ†å—
        chunks_text = self._chunk_text(
            document.content,
            chunk_strategy,
            chunk_size,
            chunk_overlap
        )
        
        # åˆ›å»ºåˆ†å—è®°å½•
        chunks = []
        for i, text in enumerate(chunks_text):
            chunk = DocumentChunk(
                document_id=document.id,
                content=text,
                chunk_index=i,
                chunk_type=chunk_strategy.value if hasattr(chunk_strategy, 'value') else str(chunk_strategy),
                char_count=len(text),
                word_count=len(text.split()),
                start_pos=document.content.find(text[:50]) if len(text) >= 50 else 0
            )
            chunk.end_pos = chunk.start_pos + len(text)
            chunks.append(chunk)
            db.add(chunk)
        
        db.commit()
        
        # åˆ·æ–°å¯¹è±¡ä»¥è·å– ID
        for chunk in chunks:
            db.refresh(chunk)
        
        return chunks
    
    def _chunk_text(
        self,
        content: str,
        strategy: ChunkStrategyEnum,
        chunk_size: int,
        chunk_overlap: int
    ) -> List[str]:
        """æ‰§è¡Œæ–‡æœ¬åˆ†å—"""
        chunker = TextChunker(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        if strategy == ChunkStrategyEnum.SEMANTIC:
            return chunker.semantic_chunking(content, language='zh')
        elif strategy == ChunkStrategyEnum.FIXED:
            return chunker.fixed_size_chunking(content)
        elif strategy == ChunkStrategyEnum.RECURSIVE:
            return chunker.recursive_chunking(content)
        elif strategy == ChunkStrategyEnum.PARAGRAPH:
            return chunker.paragraph_chunking(content)
        else:
            return chunker.semantic_chunking(content, language='zh')
    
    def _vectorize_chunks_step(
        self,
        db: Session,
        chunks: List[DocumentChunk],
        kb: KnowledgeBase
    ):
        """å‘é‡åŒ–åˆ†å—æ­¥éª¤"""
        vector_store = self.get_vector_store(kb)
        if not vector_store:
            return
        
        try:
            texts = [chunk.content for chunk in chunks]
            metadatas = [
                {
                    'chunk_id': chunk.id,
                    'document_id': chunk.document_id,
                    'chunk_index': chunk.chunk_index
                }
                for chunk in chunks
            ]
            doc_ids = [f"chunk_{chunk.id}" for chunk in chunks]
            
            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            added_ids = vector_store.add_texts(texts, metadatas, doc_ids)
            
            # æ›´æ–°åˆ†å—è®°å½•
            for chunk, vector_id in zip(chunks, added_ids):
                chunk.vector_id = vector_id
                chunk.has_embedding = True
                chunk.embedding_model = vector_store.embedding.model
            
            db.commit()
            
        except Exception as e:
            print(f"âš ï¸  å‘é‡åŒ–å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç»§ç»­å¤„ç†
    
    def _extract_entities_relations_step(
        self,
        db: Session,
        document: Document,
        chunks: List[DocumentChunk],
        kb: KnowledgeBase
    ) -> Tuple[int, int]:
        """å®ä½“å…³ç³»æå–æ­¥éª¤"""
        ner_extractor = self.get_ner_extractor(kb)
        if not ner_extractor:
            return 0, 0
        
        try:
            # å¯¹æ¯ä¸ªåˆ†å—è¿›è¡Œå®ä½“å…³ç³»æå–
            total_entities = set()
            total_relations = []
            
            for chunk in chunks:
                try:
                    # æå–å®ä½“å’Œå…³ç³»ï¼ˆä¼ å…¥ chunk_idï¼‰
                    result = ner_extractor.process_text(
                        chunk.content,
                        chunk_id=str(chunk.id),  # ä¼ å…¥ chunk_id
                        use_workflow=False  # ä½¿ç”¨ç›´æ¥æ¨¡å¼ï¼Œæ›´å¿«
                    )
                    
                    # æ›´æ–°åˆ†å—çš„å®ä½“å’Œå…³ç³»ï¼ˆå­˜å‚¨è¯¦ç»†ä¿¡æ¯ï¼‰
                    entities = [e.name for e in result.get('entities', [])]
                    relations = [
                        {
                            'subject': r.subject,
                            'subject_type': r.subject_type,
                            'predicate': r.predicate,
                            'object': r.object,
                            'object_type': r.object_type,
                            'confidence': r.confidence,
                            'chunk_ids': r.chunk_ids,
                            'contexts': r.contexts
                        }
                        for r in result.get('relations', [])
                    ]
                    
                    chunk.entities = entities
                    chunk.relations = relations
                    
                    # ç´¯è®¡ç»Ÿè®¡
                    total_entities.update(entities)
                    total_relations.extend(relations)
                    
                except Exception as e:
                    print(f"âš ï¸  åˆ†å— {chunk.id} NER å¤±è´¥: {e}")
                    continue
            
            db.commit()
            
            return len(total_entities), len(total_relations)
            
        except Exception as e:
            print(f"âš ï¸  å®ä½“å…³ç³»æå–å¤±è´¥: {e}")
            return 0, 0
    
    def _store_to_knowledge_graph_step(
        self,
        db: Session,
        chunks: List[DocumentChunk],
        kb: KnowledgeBase
    ):
        """çŸ¥è¯†å›¾è°±å­˜å‚¨æ­¥éª¤"""
        neo4j_client = self.get_neo4j_client(kb)
        if not neo4j_client:
            return
        
        try:
            # æ”¶é›†æ‰€æœ‰ä¸‰å…ƒç»„
            all_triples = []
            
            for chunk in chunks:
                relations = chunk.relations or []
                for rel in relations:
                    # ç¡®ä¿å…³ç³»æœ‰å¿…è¦çš„å­—æ®µ
                    subject = rel.get('subject', '')
                    subject_type = rel.get('subject_type', 'Concept')
                    predicate = rel.get('predicate', 'RELATES_TO')
                    object_ = rel.get('object', '')
                    object_type = rel.get('object_type', 'Concept')
                    
                    # æ„å»ºä¸‰å…ƒç»„ï¼ˆç¡®ä¿ä¸»ä½“å’Œå®¢ä½“éƒ½å­˜åœ¨ï¼‰
                    if subject and object_:
                        triple = (subject, subject_type, predicate, object_, object_type)
                        all_triples.append(triple)
            
            if all_triples:
                # æ‰¹é‡æ’å…¥åˆ° Neo4j
                result = neo4j_client.insert_triples_batch(
                    all_triples,
                    batch_size=100
                )
                print(f"   æ’å…¥ä¸‰å…ƒç»„: æˆåŠŸ {result['success']}, å¤±è´¥ {result['failed']}")
            
        except Exception as e:
            print(f"âš ï¸  çŸ¥è¯†å›¾è°±å­˜å‚¨å¤±è´¥: {e}")
    
    # ==================== æœç´¢åŠŸèƒ½ ====================
    
    def search_documents(
        self,
        db: Session,
        kb_id: int,
        query: str,
        top_k: int = 5
    ) -> List[Tuple[DocumentChunk, float]]:
        """æœç´¢æ–‡æ¡£"""
        kb = db.query(KnowledgeBase).filter(KnowledgeBase.id == kb_id).first()
        if not kb:
            return []
        
        vector_store = self.get_vector_store(kb)
        if not vector_store:
            return []
        
        try:
            results = vector_store.search(query, top_k=top_k)
            
            # è½¬æ¢ç»“æœ
            search_results = []
            for doc_metadata, score in results:
                chunk_id = doc_metadata.metadata.get('chunk_id')
                if chunk_id:
                    chunk = db.query(DocumentChunk).filter(DocumentChunk.id == chunk_id).first()
                    if chunk:
                        search_results.append((chunk, score))
            
            return search_results
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_processing_status(self, document_id: int) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£å¤„ç†çŠ¶æ€"""
        with self.processing_lock:
            if document_id in self.processing_tasks:
                task = self.processing_tasks[document_id]
                return {
                    'in_queue': True,
                    'done': task.done(),
                    'running': task.running()
                }
            return {
                'in_queue': False,
                'done': None,
                'running': False
            }
    
    def search_graph(
        self,
        db: Session,
        kb_id: int,
        query: str,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        å›¾è°±æœç´¢ï¼šåŸºäºå®ä½“å’Œå…³ç³»è¿›è¡Œæœç´¢
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            kb_id: çŸ¥è¯†åº“ID
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            å›¾è°±æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            from database.models import DocumentChunk
            
            # 1. åœ¨æ‰€æœ‰ chunk çš„ entities ä¸­æœç´¢åŒ¹é…çš„å®ä½“
            chunks = db.query(DocumentChunk).join(
                Document
            ).filter(
                Document.knowledge_base_id == kb_id
            ).all()
            # 2. æ‰¾å‡ºåŒ…å«æŸ¥è¯¢å…³é”®è¯çš„å®ä½“å’Œå…³ç³»
            entity_matches = {}  # {entity_name: {chunks: [], relations: []}}
            
            for chunk in chunks:
                entities = chunk.entities or []
                relations = chunk.relations or []
                # æ£€æŸ¥å®ä½“æ˜¯å¦åŒ¹é…
                for entity_name in entities:
                    if query.lower() in entity_name.lower() or entity_name.lower() in query.lower():
                        if entity_name not in entity_matches:
                            entity_matches[entity_name] = {
                                'chunks': [],
                                'relations': [],
                                'entity_type': 'Concept'  # é»˜è®¤ç±»å‹
                            }
                        
                        entity_matches[entity_name]['chunks'].append({
                            'chunk_id': chunk.id,
                            'document_id': chunk.document_id,
                            'content': chunk.content,
                            'chunk_index': chunk.chunk_index
                        })
                
                # æ£€æŸ¥å…³ç³»æ˜¯å¦åŒ¹é…
                for relation in relations:
                    subject = relation.get('subject', '')
                    predicate = relation.get('predicate', '')
                    object_ = relation.get('object', '')
                    
                    # å¦‚æœæŸ¥è¯¢è¯å‡ºç°åœ¨å…³ç³»çš„ä»»ä½•éƒ¨åˆ†
                    if (query.lower() in subject.lower() or 
                        query.lower() in object_.lower() or
                        query.lower() in predicate.lower()):
                        
                        # è®°å½•ç›¸å…³å®ä½“
                        for entity_name in [subject, object_]:
                            if entity_name not in entity_matches:
                                entity_matches[entity_name] = {
                                    'chunks': [],
                                    'relations': [],
                                    'entity_type': relation.get('subject_type', 'Concept') if entity_name == subject else relation.get('object_type', 'Concept')
                                }
                            
                            entity_matches[entity_name]['relations'].append({
                                'subject': subject,
                                'subject_type': relation.get('subject_type', 'Concept'),
                                'predicate': predicate,
                                'object': object_,
                                'object_type': relation.get('object_type', 'Concept'),
                                'confidence': relation.get('confidence', 0.8),
                                'chunk_id': chunk.id
                            })
                            
                            if chunk.id not in [c['chunk_id'] for c in entity_matches[entity_name]['chunks']]:
                                entity_matches[entity_name]['chunks'].append({
                                    'chunk_id': chunk.id,
                                    'document_id': chunk.document_id,
                                    'content': chunk.content,
                                    'chunk_index': chunk.chunk_index
                                })
            
            # 3. æ„å»ºå›¾è°±æœç´¢ç»“æœ
            graph_results = []
            for entity_name, data in entity_matches.items():
                # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
                relevance_score = 0.0
                
                # åç§°åŒ¹é…åº¦
                if query.lower() == entity_name.lower():
                    relevance_score += 1.0
                elif query.lower() in entity_name.lower():
                    relevance_score += 0.7
                
                # å…³ç³»æ•°é‡åŠ æƒ
                relevance_score += min(len(data['relations']) * 0.1, 0.5)
                
                # å‡ºç°é¢‘ç‡åŠ æƒ
                relevance_score += min(len(data['chunks']) * 0.05, 0.3)
                
                # æå–ç›¸å…³å®ä½“
                related_entities = []
                seen_entities = set()
                
                for rel in data['relations']:
                    # æ·»åŠ å…³ç³»ä¸­çš„å¦ä¸€ä¸ªå®ä½“
                    if rel['subject'] == entity_name and rel['object'] not in seen_entities:
                        related_entities.append({
                            'name': rel['object'],
                            'type': rel['object_type'],
                            'relation': rel['predicate']
                        })
                        seen_entities.add(rel['object'])
                    elif rel['object'] == entity_name and rel['subject'] not in seen_entities:
                        related_entities.append({
                            'name': rel['subject'],
                            'type': rel['subject_type'],
                            'relation': rel['predicate']
                        })
                        seen_entities.add(rel['subject'])
                
                graph_results.append({
                    'entity_name': entity_name,
                    'entity_type': data['entity_type'],
                    'related_entities': related_entities[:5],  # æœ€å¤š5ä¸ªç›¸å…³å®ä½“
                    'relations': data['relations'][:10],  # æœ€å¤š10ä¸ªå…³ç³»
                    'chunk_ids': [str(c['chunk_id']) for c in data['chunks']],
                    'chunks': data['chunks'],
                    'relevance_score': min(relevance_score, 1.0)
                })
            
            # 4. æŒ‰ç›¸å…³æ€§æ’åºå¹¶è¿”å› top_k
            graph_results.sort(key=lambda x: x['relevance_score'], reverse=True)
            return graph_results[:top_k]
            
        except Exception as e:
            print(f"âŒ å›¾è°±æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []


# å…¨å±€æœåŠ¡å®ä¾‹
document_service = DocumentProcessingService(max_workers=4)
