"""
åŸºäº LangGraph çš„å®ä½“å…³ç³»æå–å·¥ä½œæµ
ä½¿ç”¨ OpenAI è¿›è¡Œå®ä½“è¯†åˆ«å’Œå…³ç³»æå–ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„
"""

import os
import json
from typing import TypedDict, List, Dict, Any, Optional, Annotated
from dataclasses import dataclass, asdict, field
import operator

try:
    from langgraph.graph import StateGraph, END
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    print("âš ï¸  LangGraph æœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½ä¸å¯ç”¨")

from openai import OpenAI


class EntityType:
    """å®ä½“ç±»å‹æšä¸¾"""
    PERSON = "Person"              # äººç‰©
    ORGANIZATION = "Organization"  # ç»„ç»‡æœºæ„
    LOCATION = "Location"          # åœ°ç‚¹
    PRODUCT = "Product"            # äº§å“
    EVENT = "Event"                # äº‹ä»¶
    DATE = "Date"                  # æ—¥æœŸæ—¶é—´
    WORK = "Work"                  # ä½œå“ï¼ˆä¹¦ç±ã€ç”µå½±ç­‰ï¼‰
    CONCEPT = "Concept"            # æ¦‚å¿µ
    RESOURCE = "Resource"          # èµ„æº
    CATEGORY = "Category"          # ç±»åˆ«
    OPERATION = "Operation"        # æ“ä½œ/è¡Œä¸º
    
    @classmethod
    def all_types(cls) -> List[str]:
        """è·å–æ‰€æœ‰å®ä½“ç±»å‹"""
        return [
            cls.PERSON, cls.ORGANIZATION, cls.LOCATION, cls.PRODUCT,
            cls.EVENT, cls.DATE, cls.WORK, cls.CONCEPT,
            cls.RESOURCE, cls.CATEGORY, cls.OPERATION
        ]
    
    @classmethod
    def validate(cls, entity_type: str) -> str:
        """éªŒè¯å¹¶æ ‡å‡†åŒ–å®ä½“ç±»å‹"""
        all_types = cls.all_types()
        
        # ç²¾ç¡®åŒ¹é…
        if entity_type in all_types:
            return entity_type
        
        # å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
        entity_type_lower = entity_type.lower()
        for valid_type in all_types:
            if valid_type.lower() == entity_type_lower:
                return valid_type
        
        # éƒ¨åˆ†åŒ¹é…æˆ–æ˜ å°„
        type_mapping = {
            'people': cls.PERSON,
            'person': cls.PERSON,
            'human': cls.PERSON,
            'org': cls.ORGANIZATION,
            'company': cls.ORGANIZATION,
            'place': cls.LOCATION,
            'geo': cls.LOCATION,
            'time': cls.DATE,
            'datetime': cls.DATE,
            'book': cls.WORK,
            'movie': cls.WORK,
            'idea': cls.CONCEPT,
        }
        
        if entity_type_lower in type_mapping:
            return type_mapping[entity_type_lower]
        
        # é»˜è®¤è¿”å› Concept
        return cls.CONCEPT


@dataclass
class Entity:
    """
    å®ä½“æ•°æ®ç»“æ„ï¼ˆå¢å¼ºç‰ˆï¼‰
    
    åŒ…å«å®ä½“çš„å®Œæ•´ä¿¡æ¯å’Œå…ƒæ•°æ®
    """
    name: str                                    # å®ä½“åç§°
    entity_type: str                             # å®ä½“ç±»å‹ï¼ˆé™å®šç±»å‹ï¼‰
    chunk_ids: List[str] = field(default_factory=list)  # å‡ºç°çš„æ–‡æœ¬å—IDåˆ—è¡¨
    properties: Optional[Dict[str, Any]] = field(default_factory=dict)  # å®ä½“å±æ€§
    
    # å…ƒä¿¡æ¯
    frequency: int = 1                           # å‡ºç°é¢‘ç‡
    first_seen_chunk: Optional[str] = None       # é¦–æ¬¡å‡ºç°çš„æ–‡æœ¬å—ID
    aliases: List[str] = field(default_factory=list)  # åˆ«ååˆ—è¡¨
    description: Optional[str] = None            # å®ä½“æè¿°
    
    # ç»Ÿè®¡ä¿¡æ¯
    confidence: float = 1.0                      # ç½®ä¿¡åº¦
    importance_score: float = 0.0                # é‡è¦æ€§å¾—åˆ†
    
    def __post_init__(self):
        """éªŒè¯å¹¶æ ‡å‡†åŒ–å®ä½“ç±»å‹"""
        self.entity_type = EntityType.validate(self.entity_type)
        
        # å¦‚æœ chunk_ids éç©ºä¸” first_seen_chunk æœªè®¾ç½®
        if self.chunk_ids and not self.first_seen_chunk:
            self.first_seen_chunk = self.chunk_ids[0]
    
    def add_chunk(self, chunk_id: str):
        """æ·»åŠ æ–‡æœ¬å—ID"""
        if chunk_id not in self.chunk_ids:
            self.chunk_ids.append(chunk_id)
            self.frequency = len(self.chunk_ids)
            
            # è®¾ç½®é¦–æ¬¡å‡ºç°
            if not self.first_seen_chunk:
                self.first_seen_chunk = chunk_id
    
    def merge_with(self, other: 'Entity'):
        """
        åˆå¹¶å¦ä¸€ä¸ªå®ä½“çš„ä¿¡æ¯
        
        ç”¨äºå¤„ç†åŒä¸€å®ä½“çš„å¤šæ¬¡å‡ºç°
        """
        # åˆå¹¶ chunk_ids
        for chunk_id in other.chunk_ids:
            self.add_chunk(chunk_id)
        
        # åˆå¹¶åˆ«å
        if other.name != self.name and other.name not in self.aliases:
            self.aliases.append(other.name)
        
        for alias in other.aliases:
            if alias not in self.aliases and alias != self.name:
                self.aliases.append(alias)
        
        # åˆå¹¶å±æ€§
        if other.properties:
            self.properties.update(other.properties)
        
        # æ›´æ–°ç½®ä¿¡åº¦ï¼ˆå–å¹³å‡å€¼ï¼‰
        self.confidence = (self.confidence + other.confidence) / 2
        
        # æ›´æ–°æè¿°ï¼ˆå¦‚æœå½“å‰æ²¡æœ‰ï¼‰
        if not self.description and other.description:
            self.description = other.description
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'name': self.name,
            'entity_type': self.entity_type,
            'chunk_ids': self.chunk_ids,
            'properties': self.properties,
            'frequency': self.frequency,
            'first_seen_chunk': self.first_seen_chunk,
            'aliases': self.aliases,
            'description': self.description,
            'confidence': self.confidence,
            'importance_score': self.importance_score
        }


@dataclass
class Relation:
    """
    å…³ç³»æ•°æ®ç»“æ„ï¼ˆå¢å¼ºç‰ˆï¼‰
    
    è®°å½•å®ä½“ä¹‹é—´çš„å…³ç³»åŠå…¶å‡ºç°ä½ç½®
    """
    subject: str                                 # ä¸»ä½“å®ä½“åç§°
    subject_type: str                            # ä¸»ä½“ç±»å‹
    predicate: str                               # å…³ç³»ç±»å‹
    object: str                                  # å®¢ä½“å®ä½“åç§°
    object_type: str                             # å®¢ä½“ç±»å‹
    chunk_ids: List[str] = field(default_factory=list)  # å…³ç³»å‡ºç°çš„æ–‡æœ¬å—IDåˆ—è¡¨
    
    # å…ƒä¿¡æ¯
    confidence: float = 1.0                      # ç½®ä¿¡åº¦
    frequency: int = 1                           # å‡ºç°é¢‘ç‡
    first_seen_chunk: Optional[str] = None       # é¦–æ¬¡å‡ºç°çš„æ–‡æœ¬å—ID
    properties: Optional[Dict[str, Any]] = field(default_factory=dict)  # å…³ç³»å±æ€§
    
    # ä¸Šä¸‹æ–‡ä¿¡æ¯
    contexts: List[str] = field(default_factory=list)  # å…³ç³»å‡ºç°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µ
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        # éªŒè¯å®ä½“ç±»å‹
        self.subject_type = EntityType.validate(self.subject_type)
        self.object_type = EntityType.validate(self.object_type)
        
        # è®¾ç½®é¦–æ¬¡å‡ºç°
        if self.chunk_ids and not self.first_seen_chunk:
            self.first_seen_chunk = self.chunk_ids[0]
    
    def add_chunk(self, chunk_id: str, context: Optional[str] = None):
        """
        æ·»åŠ æ–‡æœ¬å—ID
        
        åªæœ‰å½“ä¸»ä½“ã€å®¢ä½“å’Œå…³ç³»éƒ½åœ¨è¯¥æ–‡æœ¬å—ä¸­å‡ºç°æ—¶æ‰æ·»åŠ 
        """
        if chunk_id not in self.chunk_ids:
            self.chunk_ids.append(chunk_id)
            self.frequency = len(self.chunk_ids)
            
            # è®¾ç½®é¦–æ¬¡å‡ºç°
            if not self.first_seen_chunk:
                self.first_seen_chunk = chunk_id
            
            # æ·»åŠ ä¸Šä¸‹æ–‡
            if context and context not in self.contexts:
                self.contexts.append(context)
    
    def get_relation_key(self) -> str:
        """è·å–å…³ç³»çš„å”¯ä¸€æ ‡è¯†"""
        return f"{self.subject}|{self.predicate}|{self.object}"
    
    def merge_with(self, other: 'Relation'):
        """
        åˆå¹¶å¦ä¸€ä¸ªå…³ç³»çš„ä¿¡æ¯
        
        ç”¨äºå¤„ç†åŒä¸€å…³ç³»åœ¨ä¸åŒæ–‡æœ¬å—ä¸­çš„å‡ºç°
        """
        # åˆå¹¶ chunk_ids
        for i, chunk_id in enumerate(other.chunk_ids):
            context = other.contexts[i] if i < len(other.contexts) else None
            self.add_chunk(chunk_id, context)
        
        # æ›´æ–°ç½®ä¿¡åº¦ï¼ˆå–å¹³å‡å€¼ï¼‰
        self.confidence = (self.confidence + other.confidence) / 2
        
        # åˆå¹¶å±æ€§
        if other.properties:
            self.properties.update(other.properties)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'subject': self.subject,
            'subject_type': self.subject_type,
            'predicate': self.predicate,
            'object': self.object,
            'object_type': self.object_type,
            'chunk_ids': self.chunk_ids,
            'confidence': self.confidence,
            'frequency': self.frequency,
            'first_seen_chunk': self.first_seen_chunk,
            'properties': self.properties,
            'contexts': self.contexts
        }


@dataclass
class Triple:
    """ä¸‰å…ƒç»„æ•°æ®ç»“æ„"""
    subject: str
    predicate: str
    object: str
    subject_type: str = "Entity"
    object_type: str = "Entity"


class GraphState(TypedDict):
    """LangGraph å·¥ä½œæµçŠ¶æ€"""
    text: str                                      # è¾“å…¥æ–‡æœ¬
    entities: List[Entity]                         # æå–çš„å®ä½“
    relations: List[Relation]                      # æå–çš„å…³ç³»
    triples: List[Triple]                          # ç”Ÿæˆçš„ä¸‰å…ƒç»„
    error: Optional[str]                           # é”™è¯¯ä¿¡æ¯
    metadata: Dict[str, Any]                       # å…ƒæ•°æ®
    iteration: Annotated[int, operator.add]        # è¿­ä»£æ¬¡æ•°


class EntityRelationExtractor:
    """
    å®ä½“å…³ç³»æå–å™¨ï¼ˆåŸºäº LangGraph å·¥ä½œæµï¼‰
    
    åŠŸèƒ½ï¼š
    1. ä½¿ç”¨ OpenAI æå–å®ä½“
    2. ä½¿ç”¨ OpenAI æå–å…³ç³»
    3. æ„å»ºçŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„
    4. å·¥ä½œæµçŠ¶æ€ç®¡ç†
    5. ä¸ Neo4j é›†æˆ
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "gpt-4",
        temperature: float = 0.3,
        max_retries: int = 3
    ):
        """
        åˆå§‹åŒ–å®ä½“å…³ç³»æå–å™¨
        
        Args:
            api_key: OpenAI API å¯†é’¥
            base_url: API åŸºç¡€ URLï¼ˆç”¨äºè‡ªå®šä¹‰ç«¯ç‚¹ï¼‰
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url
        self.model = model
        self.temperature = temperature
        self.max_retries = max_retries
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        if self.base_url:
            self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
        else:
            self.client = OpenAI(api_key=self.api_key)
        
        # åˆå§‹åŒ–å·¥ä½œæµ
        if LANGGRAPH_AVAILABLE:
            self.workflow = self._build_workflow()
            self.app = self.workflow.compile()
        else:
            self.workflow = None
            self.app = None
            print("âš ï¸  LangGraph å·¥ä½œæµæœªåˆå§‹åŒ–")
    
    def _build_workflow(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        workflow = StateGraph(GraphState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("extract_entities", self._extract_entities_node)
        workflow.add_node("extract_relations", self._extract_relations_node)
        workflow.add_node("build_triples", self._build_triples_node)
        workflow.add_node("validate", self._validate_node)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("extract_entities")
        
        # æ·»åŠ è¾¹
        workflow.add_edge("extract_entities", "extract_relations")
        workflow.add_edge("extract_relations", "build_triples")
        workflow.add_edge("build_triples", "validate")
        workflow.add_edge("validate", END)
        
        return workflow
    
    # ==================== LangGraph èŠ‚ç‚¹å‡½æ•° ====================
    
    def _extract_entities_node(self, state: GraphState) -> GraphState:
        """å®ä½“æå–èŠ‚ç‚¹"""
        print(f"ğŸ” [æ­¥éª¤1] å®ä½“æå–...")
        
        try:
            entities = self._extract_entities(state["text"])
            print(f"  âœ… æå–åˆ° {len(entities)} ä¸ªå®ä½“")
            
            return {
                **state,
                "entities": entities,
                "iteration": 1
            }
        except Exception as e:
            print(f"  âŒ å®ä½“æå–å¤±è´¥: {e}")
            return {
                **state,
                "entities": [],
                "error": str(e),
                "iteration": 1
            }
    
    def _extract_relations_node(self, state: GraphState) -> GraphState:
        """å…³ç³»æå–èŠ‚ç‚¹"""
        print(f"ğŸ”— [æ­¥éª¤2] å…³ç³»æå–...")
        
        try:
            relations = self._extract_relations(
                state["text"],
                state.get("entities", [])
            )
            print(f"  âœ… æå–åˆ° {len(relations)} ä¸ªå…³ç³»")
            
            return {
                **state,
                "relations": relations,
                "iteration": 1
            }
        except Exception as e:
            print(f"  âŒ å…³ç³»æå–å¤±è´¥: {e}")
            return {
                **state,
                "relations": [],
                "error": str(e),
                "iteration": 1
            }
    
    def _build_triples_node(self, state: GraphState) -> GraphState:
        """æ„å»ºä¸‰å…ƒç»„èŠ‚ç‚¹"""
        print(f"ğŸ“¦ [æ­¥éª¤3] æ„å»ºä¸‰å…ƒç»„...")
        
        relations = state.get("relations", [])
        triples = []
        
        for rel in relations:
            triple = Triple(
                subject=rel.subject,
                predicate=rel.predicate,
                object=rel.object,
                subject_type=rel.subject_type,
                object_type=rel.object_type
            )
            triples.append(triple)
        
        print(f"  âœ… ç”Ÿæˆ {len(triples)} ä¸ªä¸‰å…ƒç»„")
        
        return {
            **state,
            "triples": triples,
            "iteration": 1
        }
    
    def _validate_node(self, state: GraphState) -> GraphState:
        """éªŒè¯èŠ‚ç‚¹"""
        print(f"âœ“ [æ­¥éª¤4] éªŒè¯ç»“æœ...")
        
        # ç®€å•éªŒè¯
        entities_count = len(state.get("entities", []))
        relations_count = len(state.get("relations", []))
        triples_count = len(state.get("triples", []))
        
        print(f"  å®ä½“æ•°: {entities_count}")
        print(f"  å…³ç³»æ•°: {relations_count}")
        print(f"  ä¸‰å…ƒç»„æ•°: {triples_count}")
        
        return {
            **state,
            "metadata": {
                "entities_count": entities_count,
                "relations_count": relations_count,
                "triples_count": triples_count
            },
            "iteration": 1
        }
    
    # ==================== OpenAI è°ƒç”¨å‡½æ•° ====================
    
    def _extract_entities(self, text: str, chunk_id: Optional[str] = None) -> List[Entity]:
        """
        ä½¿ç”¨ OpenAI æå–å®ä½“
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            chunk_id: æ–‡æœ¬å—IDï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å®ä½“åˆ—è¡¨
        """
        entity_types_desc = """
å®ä½“ç±»å‹è¯´æ˜ï¼ˆå¿…é¡»ä½¿ç”¨ä»¥ä¸‹ç±»å‹ä¹‹ä¸€ï¼‰ï¼š
1. Person - äººç‰©ï¼šçœŸå®æˆ–è™šæ„çš„äººç‰©ï¼Œå¦‚"å¼ ä¸‰"ã€"çˆ±å› æ–¯å¦"
2. Organization - ç»„ç»‡æœºæ„ï¼šå…¬å¸ã€æ”¿åºœã€å­¦æ ¡ç­‰ï¼Œå¦‚"è…¾è®¯"ã€"å“ˆä½›å¤§å­¦"
3. Location - åœ°ç‚¹ï¼šåœ°ç†ä½ç½®ï¼Œå¦‚"åŒ—äº¬"ã€"é•¿æ±Ÿ"
4. Product - äº§å“ï¼šå•†å“ã€æœåŠ¡ã€è½¯ä»¶ç­‰ï¼Œå¦‚"iPhone"ã€"å¾®ä¿¡"
5. Event - äº‹ä»¶ï¼šå†å²äº‹ä»¶ã€æ´»åŠ¨ç­‰ï¼Œå¦‚"äº”å››è¿åŠ¨"ã€"å¥¥è¿ä¼š"
6. Date - æ—¥æœŸæ—¶é—´ï¼šæ—¶é—´ç‚¹æˆ–æ—¶é—´æ®µï¼Œå¦‚"2024å¹´"ã€"æ˜¥èŠ‚"
7. Work - ä½œå“ï¼šä¹¦ç±ã€ç”µå½±ã€éŸ³ä¹ç­‰åˆ›ä½œï¼Œå¦‚"çº¢æ¥¼æ¢¦"ã€"è’™å¨œä¸½è"
8. Concept - æ¦‚å¿µï¼šæŠ½è±¡æ¦‚å¿µã€ç†è®ºç­‰ï¼Œå¦‚"äººå·¥æ™ºèƒ½"ã€"é‡å­åŠ›å­¦"
9. Resource - èµ„æºï¼šè‡ªç„¶èµ„æºã€æ•°æ®ç­‰ï¼Œå¦‚"çŸ³æ²¹"ã€"æ•°æ®é›†"
10. Category - ç±»åˆ«ï¼šåˆ†ç±»ã€ç±»å‹ç­‰ï¼Œå¦‚"ç¼–ç¨‹è¯­è¨€"ã€"å“ºä¹³åŠ¨ç‰©"
11. Operation - æ“ä½œ/è¡Œä¸ºï¼šåŠ¨ä½œã€æµç¨‹ç­‰ï¼Œå¦‚"æœºå™¨å­¦ä¹ "ã€"æ•°æ®å¤„ç†"
"""
        
        prompt = f"""
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æ‰€æœ‰å®ä½“ï¼Œå¹¶æŒ‰ç…§ JSON æ ¼å¼è¿”å›ã€‚

æ–‡æœ¬ï¼š
{text}

{entity_types_desc}

è¦æ±‚ï¼š
1. å‡†ç¡®è¯†åˆ«æ–‡æœ¬ä¸­çš„å®ä½“
2. entity_type å¿…é¡»æ˜¯ä¸Šè¿°11ç§ç±»å‹ä¹‹ä¸€ï¼ˆä¸¥æ ¼åŒ¹é…è‹±æ–‡åç§°ï¼‰
3. æä¾›å®ä½“çš„æè¿°ã€åˆ«åç­‰å±æ€§
4. å¯¹äºé‡è¦å®ä½“ï¼Œå¯ä»¥æ·»åŠ  description å­—æ®µ
5. ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼

è¾“å‡ºæ ¼å¼ï¼š
{{
    "entities": [
        {{
            "name": "å®ä½“åç§°",
            "entity_type": "Person|Organization|Location|Product|Event|Date|Work|Concept|Resource|Category|Operation",
            "description": "å®ä½“æè¿°ï¼ˆå¯é€‰ï¼‰",
            "aliases": ["åˆ«å1", "åˆ«å2"],
            "properties": {{
                "key1": "value1",
                "key2": "value2"
            }},
            "confidence": 0.95
        }}
    ]
}}

ç¤ºä¾‹ï¼š
{{
    "entities": [
        {{
            "name": "å¼ ä¸‰",
            "entity_type": "Person",
            "description": "è½¯ä»¶å·¥ç¨‹å¸ˆ",
            "aliases": ["å°å¼ "],
            "properties": {{"èŒä¸š": "å·¥ç¨‹å¸ˆ", "å…¬å¸": "è…¾è®¯"}},
            "confidence": 0.98
        }},
        {{
            "name": "äººå·¥æ™ºèƒ½",
            "entity_type": "Concept",
            "description": "è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
            "properties": {{"é¢†åŸŸ": "è®¡ç®—æœºç§‘å­¦"}},
            "confidence": 1.0
        }}
    ]
}}

è¯·åªè¿”å› JSONï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜æ–‡å­—ã€‚
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®ä½“è¯†åˆ«ä¸“å®¶ï¼Œæ“…é•¿ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–çš„å®ä½“ä¿¡æ¯ã€‚å¿…é¡»ä¸¥æ ¼éµå®ˆå®ä½“ç±»å‹çš„å®šä¹‰ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            entities = []
            
            for entity_data in result.get("entities", []):
                # åˆ›å»ºå®ä½“
                entity = Entity(
                    name=entity_data.get("name", ""),
                    entity_type=entity_data.get("entity_type", "Concept"),
                    chunk_ids=[chunk_id] if chunk_id else [],
                    properties=entity_data.get("properties", {}),
                    description=entity_data.get("description"),
                    aliases=entity_data.get("aliases", []),
                    confidence=entity_data.get("confidence", 0.8),
                    first_seen_chunk=chunk_id
                )
                entities.append(entity)
            
            return entities
            
        except Exception as e:
            print(f"å®ä½“æå–é”™è¯¯: {e}")
            return []
    
    def _extract_relations(
        self,
        text: str,
        entities: List[Entity],
        chunk_id: Optional[str] = None
    ) -> List[Relation]:
        """
        ä½¿ç”¨ OpenAI æå–å…³ç³»
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            entities: å·²æå–çš„å®ä½“åˆ—è¡¨
            chunk_id: æ–‡æœ¬å—IDï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å…³ç³»åˆ—è¡¨
        """
        # æ„å»ºå®ä½“åˆ—è¡¨å­—ç¬¦ä¸²
        entity_names = [e.name for e in entities]
        entity_info = "\n".join([f"- {e.name} ({e.entity_type})" for e in entities])
        
        prompt = f"""
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶æŒ‰ç…§ JSON æ ¼å¼è¿”å›ã€‚

æ–‡æœ¬ï¼š
{text}

å·²è¯†åˆ«çš„å®ä½“ï¼š
{entity_info}

è¦æ±‚ï¼š
1. åªæå–ä¸Šè¿°å®ä½“ä¹‹é—´çš„å…³ç³»
2. å…³ç³»åº”è¯¥æ˜¯æœ‰æ„ä¹‰çš„åŠ¨ä½œã€çŠ¶æ€æˆ–è¿æ¥
3. ä¸ºæ¯ä¸ªå…³ç³»æä¾›ç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´ï¼‰
4. æå–å…³ç³»æ‰€åœ¨çš„ä¸Šä¸‹æ–‡å¥å­
5. subject_type å’Œ object_type å¿…é¡»ä¸å·²è¯†åˆ«å®ä½“çš„ç±»å‹ä¸€è‡´
6. ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼

è¾“å‡ºæ ¼å¼ï¼š
{{
    "relations": [
        {{
            "subject": "ä¸»ä½“å®ä½“åç§°",
            "subject_type": "ä¸»ä½“ç±»å‹",
            "predicate": "å…³ç³»ç±»å‹",
            "object": "å®¢ä½“å®ä½“åç§°",
            "object_type": "å®¢ä½“ç±»å‹",
            "confidence": 0.95,
            "context": "å…³ç³»æ‰€åœ¨çš„å¥å­æˆ–çŸ­è¯­",
            "properties": {{
                "description": "å…³ç³»æè¿°",
                "time": "æ—¶é—´ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰"
            }}
        }}
    ]
}}

å¸¸è§å…³ç³»ç±»å‹ç¤ºä¾‹ï¼š
- äººç‰©å…³ç³»ï¼šæœ‹å‹ã€åŒäº‹ã€å®¶äººã€ä¸Šä¸‹çº§ã€å¸ˆç”Ÿ
- æ‰€å±å…³ç³»ï¼šå±äºã€éš¶å±ã€åŒ…å«ã€æ‹¥æœ‰
- è¡Œä¸ºå…³ç³»ï¼šåˆ›å»ºã€å‘æ˜ã€æ’°å†™ã€é¢†å¯¼ã€å¼€å‘
- ç‰¹å¾å…³ç³»ï¼šå…·æœ‰ã€è¡¨ç°ã€å±•ç¤ºã€æ˜¯
- ä½ç½®å…³ç³»ï¼šä½äºã€æ¥è‡ªã€åœ¨
- æ—¶é—´å…³ç³»ï¼šå‘ç”Ÿäºã€å¼€å§‹äºã€ç»“æŸäº

ç¤ºä¾‹ï¼š
{{
    "relations": [
        {{
            "subject": "å¼ ä¸‰",
            "subject_type": "Person",
            "predicate": "å·¥ä½œäº",
            "object": "è…¾è®¯",
            "object_type": "Organization",
            "confidence": 0.95,
            "context": "å¼ ä¸‰åœ¨è…¾è®¯å…¬å¸å·¥ä½œ",
            "properties": {{"èŒä½": "å·¥ç¨‹å¸ˆ"}}
        }}
    ]
}}

è¯·åªè¿”å› JSONï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜æ–‡å­—ã€‚
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…³ç³»æŠ½å–ä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«å®ä½“ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            relations = []
            
            for rel_data in result.get("relations", []):
                # æå–ä¸Šä¸‹æ–‡
                context = rel_data.get("context", "")
                
                # åˆ›å»ºå…³ç³»
                relation = Relation(
                    subject=rel_data.get("subject", ""),
                    subject_type=rel_data.get("subject_type", "Concept"),
                    predicate=rel_data.get("predicate", "RELATES_TO"),
                    object=rel_data.get("object", ""),
                    object_type=rel_data.get("object_type", "Concept"),
                    chunk_ids=[chunk_id] if chunk_id else [],
                    confidence=rel_data.get("confidence", 0.8),
                    first_seen_chunk=chunk_id,
                    properties=rel_data.get("properties", {}),
                    contexts=[context] if context else []
                )
                relations.append(relation)
            
            return relations
            
        except Exception as e:
            print(f"å…³ç³»æå–é”™è¯¯: {e}")
            return []
    
    # ==================== ä¸»è¦æ¥å£ ====================
    
    def process_text(
        self,
        text: str,
        chunk_id: Optional[str] = None,
        use_workflow: bool = True
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æœ¬ï¼Œæå–å®ä½“å’Œå…³ç³»
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            chunk_id: æ–‡æœ¬å—IDï¼ˆå¯é€‰ï¼‰
            use_workflow: æ˜¯å¦ä½¿ç”¨ LangGraph å·¥ä½œæµ
            
        Returns:
            åŒ…å«å®ä½“ã€å…³ç³»å’Œä¸‰å…ƒç»„çš„å­—å…¸
        """
        if use_workflow and self.app is not None:
            # ä½¿ç”¨ LangGraph å·¥ä½œæµ
            initial_state = {
                "text": text,
                "entities": [],
                "relations": [],
                "triples": [],
                "error": None,
                "metadata": {},
                "iteration": 0
            }
            
            print("ğŸš€ å¯åŠ¨ LangGraph å·¥ä½œæµ...")
            print("=" * 60)
            
            final_state = self.app.invoke(initial_state)
            
            print("=" * 60)
            print("âœ… å·¥ä½œæµå®Œæˆ\n")
            
            return {
                "entities": final_state.get("entities", []),
                "relations": final_state.get("relations", []),
                "triples": final_state.get("triples", []),
                "metadata": final_state.get("metadata", {}),
                "error": final_state.get("error")
            }
        else:
            # ç›´æ¥è°ƒç”¨ï¼ˆä¸ä½¿ç”¨å·¥ä½œæµï¼‰
            print("ğŸ”„ ç›´æ¥å¤„ç†æ¨¡å¼...")
            
            entities = self._extract_entities(text, chunk_id)
            relations = self._extract_relations(text, entities, chunk_id)
            
            triples = []
            for rel in relations:
                triple = Triple(
                    subject=rel.subject,
                    predicate=rel.predicate,
                    object=rel.object,
                    subject_type=rel.subject_type,
                    object_type=rel.object_type
                )
                triples.append(triple)
            
            return {
                "entities": entities,
                "relations": relations,
                "triples": triples,
                "metadata": {
                    "entities_count": len(entities),
                    "relations_count": len(relations),
                    "triples_count": len(triples)
                }
            }
    
    def process_chunks(
        self,
        chunks: List[str],
        chunk_ids: Optional[List[str]] = None,
        batch_size: int = 5
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†æ–‡æœ¬å—ï¼ˆæ”¯æŒå®ä½“å’Œå…³ç³»åˆå¹¶ï¼‰
        
        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨
            chunk_ids: æ–‡æœ¬å—IDåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œä¸chunkså¯¹åº”ï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            æ±‡æ€»çš„æå–ç»“æœï¼ˆåŒ…å«åˆå¹¶çš„å®ä½“å’Œå…³ç³»ï¼‰
        """
        # å¦‚æœæ²¡æœ‰æä¾› chunk_idsï¼Œè‡ªåŠ¨ç”Ÿæˆ
        if not chunk_ids:
            chunk_ids = [f"chunk_{i}" for i in range(len(chunks))]
        
        # ä½¿ç”¨å­—å…¸å­˜å‚¨å®ä½“å’Œå…³ç³»ï¼Œæ–¹ä¾¿åˆå¹¶
        entity_dict = {}  # key: (name, entity_type) -> Entity
        relation_dict = {}  # key: (subject, predicate, object) -> Relation
        all_triples = []
        
        total = len(chunks)
        
        for i in range(0, total, batch_size):
            batch_chunks = chunks[i:i + batch_size]
            batch_ids = chunk_ids[i:i + batch_size]
            
            print(f"\nå¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(total + batch_size - 1)//batch_size}")
            
            for j, (chunk_text, chunk_id) in enumerate(zip(batch_chunks, batch_ids)):
                print(f"\n--- å— {i+j+1}/{total} (ID: {chunk_id}) ---")
                
                # å¤„ç†å•ä¸ªå—
                result = self.process_text(chunk_text, chunk_id=chunk_id, use_workflow=False)
                
                # åˆå¹¶å®ä½“
                for entity in result["entities"]:
                    key = (entity.name, entity.entity_type)
                    
                    if key in entity_dict:
                        # å®ä½“å·²å­˜åœ¨ï¼Œåˆå¹¶ä¿¡æ¯
                        entity_dict[key].merge_with(entity)
                    else:
                        # æ–°å®ä½“
                        entity_dict[key] = entity
                
                # åˆå¹¶å…³ç³»
                for relation in result["relations"]:
                    key = (relation.subject, relation.predicate, relation.object)
                    
                    if key in relation_dict:
                        # å…³ç³»å·²å­˜åœ¨ï¼Œåˆå¹¶ä¿¡æ¯
                        relation_dict[key].merge_with(relation)
                    else:
                        # æ–°å…³ç³»
                        relation_dict[key] = relation
                
                # æ”¶é›†ä¸‰å…ƒç»„
                all_triples.extend(result["triples"])
        
        # è½¬æ¢ä¸ºåˆ—è¡¨
        merged_entities = list(entity_dict.values())
        merged_relations = list(relation_dict.values())
        
        # æŒ‰é¢‘ç‡æ’åºï¼ˆé‡è¦æ€§ï¼‰
        merged_entities.sort(key=lambda e: (e.frequency, e.confidence), reverse=True)
        merged_relations.sort(key=lambda r: (r.frequency, r.confidence), reverse=True)
        
        # è®¡ç®—é‡è¦æ€§å¾—åˆ†
        if merged_entities:
            max_frequency = max(e.frequency for e in merged_entities)
            for entity in merged_entities:
                # é‡è¦æ€§ = (é¢‘ç‡æƒé‡ * 0.7) + (ç½®ä¿¡åº¦æƒé‡ * 0.3)
                freq_score = entity.frequency / max_frequency if max_frequency > 0 else 0
                entity.importance_score = (freq_score * 0.7) + (entity.confidence * 0.3)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å¤„ç†å®Œæˆï¼š")
        print(f"   æ€»æ–‡æœ¬å—æ•°: {total}")
        print(f"   å”¯ä¸€å®ä½“æ•°: {len(merged_entities)}")
        print(f"   å”¯ä¸€å…³ç³»æ•°: {len(merged_relations)}")
        print(f"   ä¸‰å…ƒç»„æ•°: {len(all_triples)}")
        print(f"{'='*60}\n")
        
        return {
            "entities": merged_entities,
            "relations": merged_relations,
            "triples": all_triples,
            "metadata": {
                "total_chunks": total,
                "entities_count": len(merged_entities),
                "relations_count": len(merged_relations),
                "triples_count": len(all_triples),
                "processed_chunk_ids": chunk_ids
            }
        }
    
    def to_neo4j_format(
        self,
        result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        è½¬æ¢ä¸º Neo4j å…¼å®¹æ ¼å¼
        
        Args:
            result: æå–ç»“æœ
            
        Returns:
            Neo4j æ ¼å¼çš„æ•°æ®
        """
        from utils.neo4j import Triple as Neo4jTriple, Entity as Neo4jEntity
        
        # è½¬æ¢å®ä½“
        neo4j_entities = []
        for entity in result["entities"]:
            neo4j_entities.append(
                Neo4jEntity(
                    name=entity.name,
                    label=entity.entity_type,
                    properties=entity.properties
                )
            )
        
        # è½¬æ¢ä¸‰å…ƒç»„
        neo4j_triples = []
        for triple in result["triples"]:
            neo4j_triples.append(
                Neo4jTriple(
                    subject=triple.subject,
                    subject_label=triple.subject_type,
                    predicate=triple.predicate,
                    object=triple.object,
                    object_label=triple.object_type
                )
            )
        
        return {
            "entities": neo4j_entities,
            "triples": neo4j_triples
        }
    
    def visualize_graph(
        self,
        result: Dict[str, Any],
        output_file: str = "knowledge_graph.json"
    ):
        """
        å¯è§†åŒ–çŸ¥è¯†å›¾è°±ï¼ˆå¯¼å‡ºä¸º JSONï¼‰
        
        Args:
            result: æå–ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # æ„å»ºèŠ‚ç‚¹å’Œè¾¹
        nodes = []
        edges = []
        
        # æ·»åŠ å®ä½“èŠ‚ç‚¹
        for i, entity in enumerate(result["entities"]):
            nodes.append({
                "id": f"entity_{i}",
                "label": entity.name,
                "type": entity.entity_type,
                "properties": entity.properties or {}
            })
        
        # æ·»åŠ å…³ç³»è¾¹
        for i, relation in enumerate(result["relations"]):
            edges.append({
                "id": f"relation_{i}",
                "source": relation.subject,
                "target": relation.object,
                "label": relation.predicate,
                "confidence": relation.confidence
            })
        
        graph_data = {
            "nodes": nodes,
            "edges": edges,
            "metadata": result.get("metadata", {})
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(graph_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š å›¾è°±å·²å¯¼å‡ºåˆ°: {output_file}")


def test_entity_relation_extractor():
    """æµ‹è¯•å‡½æ•°"""
    print("=" * 80)
    print("å®ä½“å…³ç³»æå–å·¥ä½œæµæµ‹è¯•")
    print("=" * 80)
    
    # æ£€æŸ¥ API å¯†é’¥
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("\nâš ï¸  è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("ç¤ºä¾‹: export OPENAI_API_KEY='your-api-key'")
        return
    
    try:
        # åˆå§‹åŒ–æå–å™¨
        print("\nåˆå§‹åŒ–æå–å™¨...")
        extractor = EntityRelationExtractor(
            model="gpt-4",  # æˆ– "gpt-3.5-turbo"
            temperature=0.3
        )
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            """
å¼ ä¸‰æ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆï¼Œä»–åœ¨è…¾è®¯å…¬å¸å·¥ä½œã€‚å¼ ä¸‰ç²¾é€š Python å’Œæœºå™¨å­¦ä¹ ï¼Œ
æœ€è¿‘ä»–æ­£åœ¨å¼€å‘ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æ¨èç³»ç»Ÿã€‚è¿™ä¸ªé¡¹ç›®ç”±æå››é¢†å¯¼ï¼Œ
æå››æ˜¯è…¾è®¯ AI å®éªŒå®¤çš„ä¸»ä»»ã€‚
            """.strip(),
            
            """
ã€Šçº¢æ¥¼æ¢¦ã€‹æ˜¯ä¸­å›½å¤å…¸å››å¤§åè‘—ä¹‹ä¸€ï¼Œç”±æ›¹é›ªèŠ¹åˆ›ä½œäºæ¸…æœã€‚
è¿™éƒ¨å°è¯´ä¸»è¦æå†™äº†è´¾å®ç‰ã€æ—é»›ç‰å’Œè–›å®é’—ä¹‹é—´çš„çˆ±æƒ…æ•…äº‹ã€‚
è´¾å®ç‰æ˜¯è´¾åºœçš„å…¬å­ï¼Œæ—é»›ç‰æ˜¯ä»–çš„è¡¨å¦¹ã€‚
            """.strip()
        ]
        
        # æµ‹è¯•1: å•æ–‡æœ¬å¤„ç†
        print("\n" + "=" * 80)
        print("æµ‹è¯•1: å•æ–‡æœ¬å¤„ç†")
        print("=" * 80)
        
        result = extractor.process_text(test_texts[0], use_workflow=True)
        
        print("\nğŸ“‹ æå–ç»“æœ:")
        print(f"\nå®ä½“ ({len(result['entities'])} ä¸ª):")
        for entity in result['entities']:
            print(f"  - {entity.name} ({entity.entity_type})")
        
        print(f"\nå…³ç³» ({len(result['relations'])} ä¸ª):")
        for rel in result['relations']:
            print(f"  - {rel.subject} --[{rel.predicate}]--> {rel.object} (ç½®ä¿¡åº¦: {rel.confidence})")
        
        print(f"\nä¸‰å…ƒç»„ ({len(result['triples'])} ä¸ª):")
        for triple in result['triples']:
            print(f"  - ({triple.subject}, {triple.predicate}, {triple.object})")
        
        # æµ‹è¯•2: æ‰¹é‡å¤„ç†
        print("\n" + "=" * 80)
        print("æµ‹è¯•2: æ‰¹é‡å¤„ç†")
        print("=" * 80)
        
        batch_result = extractor.process_chunks(test_texts, batch_size=2)
        
        print(f"\nğŸ“Š æ‰¹é‡å¤„ç†ç»Ÿè®¡:")
        print(f"  æ€»å—æ•°: {batch_result['metadata']['total_chunks']}")
        print(f"  å®ä½“æ•°: {batch_result['metadata']['entities_count']}")
        print(f"  å…³ç³»æ•°: {batch_result['metadata']['relations_count']}")
        print(f"  ä¸‰å…ƒç»„æ•°: {batch_result['metadata']['triples_count']}")
        
        # æµ‹è¯•3: å¯¼å‡ºå¯è§†åŒ–
        print("\n" + "=" * 80)
        print("æµ‹è¯•3: å¯¼å‡ºå›¾è°±")
        print("=" * 80)
        
        extractor.visualize_graph(result, "test_knowledge_graph.json")
        
        # æµ‹è¯•4: Neo4j æ ¼å¼è½¬æ¢
        print("\n" + "=" * 80)
        print("æµ‹è¯•4: Neo4j æ ¼å¼è½¬æ¢")
        print("=" * 80)
        
        neo4j_data = extractor.to_neo4j_format(result)
        print(f"  è½¬æ¢äº† {len(neo4j_data['entities'])} ä¸ªå®ä½“")
        print(f"  è½¬æ¢äº† {len(neo4j_data['triples'])} ä¸ªä¸‰å…ƒç»„")
        
        print("\n" + "=" * 80)
        print("âœ… æµ‹è¯•å®Œæˆï¼")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        
        print("\nğŸ’¡ æ•…éšœæ’æŸ¥:")
        print("1. æ£€æŸ¥ OPENAI_API_KEY æ˜¯å¦æ­£ç¡®è®¾ç½®")
        print("2. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
        print("3. æ£€æŸ¥ API é…é¢æ˜¯å¦å……è¶³")
        print("4. å®‰è£…ä¾èµ–: pip install langgraph openai")


if __name__ == "__main__":
    test_entity_relation_extractor()

